{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuentes:\n",
    "\n",
    "Comenzar por youtube...\n",
    "\n",
    "https://www.youtube.com/watch?v=9zhrxE5PQgY\n",
    "\n",
    "https://www.youtube.com/watch?v=l4X-kZjl1gs\n",
    "\n",
    "Seguir por la teoria:\n",
    "\n",
    "http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/\n",
    "\n",
    "http://arunmallya.github.io/writeups/nn/lstm/index.html#/2\n",
    "\n",
    "Y terminamos en el codigo:\n",
    "\n",
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "https://github.com/nicholaslocascio/bcs-lstm\n",
    "\n",
    "https://towardsdatascience.com/lstm-by-example-using-tensorflow-feb0c1968537"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empecemos..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero en todo script de python comenzamos por los import, que es como un include en C, le decimos al interprete donde encontrar las librerias y como las llamamos.\n",
    "\n",
    "En este caso, por ser un Jupyter Notebook, le damos dos ordenes distintas al resto, que son %reset y %matplotlib inline. Estas instrucciones son para resetear las variables (en caso de querer volver a ejecutar) y de graficar sobre el mismo notebook, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "%matplotlib inline\n",
    "\n",
    "# Importamos tensoflow y le asignamos el nombre \"tf\"\n",
    "import tensorflow as tf\n",
    "# Importamos numpy (libreria de calculo) y le asignamos el nombre \"np\"\n",
    "import numpy as np\n",
    "# Importamos unicodecsv (libreria de lectura de csv que soporta utf-8, osea acentos) y le asignamos el nombre \"csv\"\n",
    "import unicodecsv as csv\n",
    "# Importamos matplotlib (libreria para hacer plots como matlab) y le asignamos el nombre \"plt\"\n",
    "import matplotlib.pyplot as plt\n",
    "# Más imports...\n",
    "import collections\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Todo esto es para poder ver correctamente las listas de palabras con acentos cuando son impresas en pantalla.\n",
    "# Solo cosmético\n",
    "import sys\n",
    "default_stdout = sys.stdout\n",
    "default_stderr = sys.stderr\n",
    "reload(sys)\n",
    "sys.stdout = default_stdout\n",
    "sys.stderr = default_stderr\n",
    "sys.setdefaultencoding('utf-8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de Parametros "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para que sea más prolijo todas las definiciones las ponemos acá. Pueden declararse donde sea las variables, pero estos van a ser las perillas que nosotros vamos a querer ajustar generalemente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unidades internas de mis celdas LTMS\n",
    "LTMS_internal_units_1 = 512\n",
    "LTMS_internal_units_2 = 512\n",
    "\n",
    "# Parametro de velocidad de aprendizaje\n",
    "velocidad_aprendizaje = 0.001\n",
    "\n",
    "# Iteraciones a realizar\n",
    "iteraciones_entrenamiento = 300000\n",
    "# Tamaño de minibatch\n",
    "tam_minibatch = 10\n",
    "# Cada cuanto vamos a escribir información en pantalla\n",
    "mostrar_info_cada = 5000\n",
    "# Cada cuanto vamos a escribir información en el tensorboard\n",
    "procesar_info_cada = 100\n",
    "\n",
    "# Este es el fulano al cual queremos inmitar\n",
    "ARCH_TWEETS = \"elonmusk_tweets.csv\"\n",
    "\n",
    "# Palabras de entrada a la red. Este parametro está para que sea facil de interpretar todo, \n",
    "# pero veremos que no hace falta...\n",
    "palabras_entrada = 3\n",
    "# Otra utilidad adicional, totalmente vacía de justificación, \n",
    "# es ponerlo en cero para calcularlo despues con el diccionario\n",
    "\n",
    "# Este parametro es importante, va a definir cuantas veces se debe repetir una palabra \n",
    "# en mi base datos original para que sea utilizada en el diccionario final.\n",
    "# El problema es simple, si alguién alguna vez se encontró tan sorprendido por algo que tuvo que decir\n",
    "# \"Fué un momento inefable\"\n",
    "# La probabilidad que aprendamos a utilizar la palabra \"inefable\" sin aprender de memoria la frace anterior\n",
    "# es realmente baja... Así como sucede con esa palabra sucede con muchas más, lo que hace que nuestra salida\n",
    "# tenga una dimención muy grande y contemos con poca información para generalizar sus usos. \n",
    "# Por lo que nos conviene quitarlo del medio...\n",
    "min_repeticiones = 5\n",
    "\n",
    "# Solo para el test...\n",
    "MODELO_RESTORE= \"./Salidas/elonmusk_tweets_Final_bot.ckpt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acá declaramos todas las funciones que vallamos a utilizar. No es necesario que esten acá, pueden declararse en cualquier momento ANTES de ser utilizadas. Pueden ir a archivos aparte también y ser importadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para armar un diccionario directo (Palabra -> Codigo) e inverso (Codigo -> Palabra) a partir de una lista\n",
    "\n",
    "def build_dataset(words):\n",
    "    count = collections.Counter(words).most_common()\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, reverse_dictionary\n",
    "\n",
    "# Función para hacer un histograma de las palabras incluidas en los tweets\n",
    "\n",
    "def histograma_diccionario(palabras, diccionario, min_repeticiones_plot = 0, PLOT = False):\n",
    "    # Saco el tamaño del vocabulario\n",
    "    tam_vocabulario = len(diccionario)\n",
    "    # Reservo memoria con un bin por palabra\n",
    "    bines = np.zeros(tam_vocabulario)\n",
    "    # Recorro todas las palabras y sumo un poroto cada vez que encuentro una repetición.\n",
    "    for i in range(0,len(palabras)):\n",
    "        # Busco el código de la palabra en el diccionario y sumo poroto ahí.\n",
    "        bines[diccionario[ str(palabras[i])]] += 1\n",
    "        \n",
    "    # Si se indica se plotea el histograma, para hacerlo más interpretable se pueden plotear solo aquellos bines\n",
    "    # con min_repeticiones_plot repeticiones. \n",
    "    # El histograma esta ordenado de mayor a menor, esa es la mágia de la función \"dict\"\n",
    "    if PLOT:\n",
    "        plt.bar(range(0,len(bines[bines>min_repeticiones_plot])), bines[bines>min_repeticiones_plot], color='g')\n",
    "        plt.show()\n",
    "\n",
    "    return bines\n",
    "\n",
    "# Funciones para TensorFlow. \n",
    "\n",
    "# Declaran pesos y biases y los inicializan con una desitribución dada.\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# Función que calcula mucha info de las variables para TensoBoard\n",
    "def variable_summaries(var):\n",
    "  #\"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "  with tf.name_scope('summaries'):\n",
    "    mean = tf.reduce_mean(var)\n",
    "    tf.summary.scalar('mean', mean)\n",
    "    with tf.name_scope('stddev'):\n",
    "        stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "    tf.summary.scalar('stddev', stddev)\n",
    "    tf.summary.scalar('max', tf.reduce_max(var))\n",
    "    tf.summary.scalar('min', tf.reduce_min(var))\n",
    "    tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construcción de entrada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura del archivo de tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras únicas dentro del vocabulario: 5739\n",
      "Palabras totales dentro de la base de datos: 36848\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIEkdMwQkqWOGgCR1zBCQpI4ZApLUsf8PHKZ18g4DLPsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f20a0d0a290>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lista vacia donde voy a meter los tweets\n",
    "palabras = list()\n",
    "# Abro el archivo para leerlo\n",
    "with open(ARCH_TWEETS, 'rb') as csvfile:\n",
    "    # Leo usando la funcion de lectura de csv\n",
    "    spamreader = csv.reader(csvfile, encoding='utf-8', delimiter=',')\n",
    "    # Una vez leido quiero separalo en palabras, así que empiezo por separalo en lineas (o tweets)\n",
    "    for row in spamreader:\n",
    "        # Y a cada linea la separo en palabras\n",
    "        for palabra in row:\n",
    "            palabras.append(palabra.encode('utf-8'))\n",
    "\n",
    "# Convierto la linea en un array de numpy\n",
    "palabras = np.asarray(palabras)\n",
    "# Y lo transformo a un vector\n",
    "palabras = np.reshape(palabras, [-1, ])\n",
    "\n",
    "# Armo el diccionario directo e inverso\n",
    "diccionario, diccionario_inverso = build_dataset(palabras)\n",
    "\n",
    "# Saco info y la imprimo\n",
    "tam_vocabulario = len(diccionario)\n",
    "cant_palabras = len(palabras)\n",
    "print('Palabras únicas dentro del vocabulario: %d'%tam_vocabulario)\n",
    "print('Palabras totales dentro de la base de datos: %d'%cant_palabras)\n",
    "\n",
    "# Ploteo el histograma\n",
    "bines = histograma_diccionario(palabras, diccionario, min_repeticiones, PLOT = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras únicas dentro del vocabulario común: 647\n",
      "Palabras totales dentro de la base de datos final: 4095\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndc6gl6TOGfSS1DmDXpI69/8Bx3MHQgtcSwoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2073d45c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Defino cual es la palabra limite a conservar\n",
    "# Recordado que el diccionario ordena por repetición, cualquier paralabra con código superior\n",
    "# a la palabra con el límite de repeticiones tendrá menos repeticiones.\n",
    "bines_aux = bines[bines>min_repeticiones]\n",
    "limit_palabra = len(bines_aux)\n",
    "\n",
    "# Volvemos a cargar el archivo pero ahora vamos a recortar las oraciones con\n",
    "# menos palabras que las elegidas\n",
    "\n",
    "# Lista auxiliar que conserva el largo de oraciones\n",
    "largo_oraciones = list()\n",
    "# Lista con las palabras con la cantidad de repeticiones deseadas\n",
    "palabras_comunes = list()\n",
    "# Abro el archivo\n",
    "with open(ARCH_TWEETS, 'rb') as csvfile:\n",
    "    # Leo el CSV nuevamente\n",
    "    spamreader = csv.reader(csvfile, encoding='utf-8', delimiter=',')\n",
    "    # Analizo cada linea o tweet\n",
    "    for row in spamreader:\n",
    "        # Lista vacia para contener las distintas oraciones dentro del tweet\n",
    "        row_of_rows = list()\n",
    "        # Analizo si hay un punto, si lo hay es que hay mas de una oración en el tweet\n",
    "        if '.' in row:\n",
    "            # Si hay punto busco donde esta\n",
    "            puntos_enc = 0\n",
    "            for palabra in row:\n",
    "                if '.' in palabra:\n",
    "                    puntos_enc += 1\n",
    "            # Por cada punto hago una enntrada en row_of_rows, separando el tweet en oraciones\n",
    "            palabra_idx = -1\n",
    "            for i in range(0,puntos_enc):\n",
    "                row_aux = list()\n",
    "                while True:\n",
    "                    palabra_idx += 1\n",
    "                    row_aux.append(row[palabra_idx])\n",
    "                    if '.' in row[palabra_idx]:\n",
    "                        break\n",
    "                row_of_rows.append(row_aux)\n",
    "\n",
    "        # Si no hay puntos, copio la unica oracion del tweet a row_of_rows\n",
    "        else:\n",
    "            row_of_rows.append(row)\n",
    "\n",
    "        # Ahora analizo cada oracion dentro de row_of_rows\n",
    "        for row_crop in row_of_rows:\n",
    "            # Si es una linea vacia la descarto (por las dudas esta esto)\n",
    "            if len(row_crop) == 0:\n",
    "                continue\n",
    "            # Agrego el largo de la oración a la lista\n",
    "            largo_oraciones.append(len(row_crop))\n",
    "            # Checkeo si la oración contiene solo palabras con más repeticiones de las pedidas\n",
    "            copiar = True\n",
    "            for palabra in row_crop:\n",
    "                # Si la codificación es mayor al limite, es una palabra con pocas repeticiones\n",
    "                if diccionario[ str(palabra)] > limit_palabra:\n",
    "                    copiar = False\n",
    "            # Si todas las palabras de la oración cumplen, entonces copio la oración a la base de datos final\n",
    "            if copiar:\n",
    "                for palabra in row_crop:        \n",
    "                    palabras_comunes.append(palabra.encode('utf-8'))\n",
    "\n",
    "# Paso nuevamente a numpy y transformo en vector\n",
    "palabras_comunes = np.asarray(palabras_comunes)\n",
    "palabras_comunes = np.reshape(palabras_comunes, [-1, ])\n",
    "\n",
    "# Genero un nuevo diccionario que solo contiene palabras \"comunes\"\n",
    "diccionario_comun, diccionario_inverso_comun = build_dataset(palabras_comunes)\n",
    "\n",
    "# Saco info y la imprimo\n",
    "tam_vocabulario_comun = len(diccionario_comun)\n",
    "cant_palabras_comunes = len(palabras_comunes)\n",
    "print('Palabras únicas dentro del vocabulario común: %d'%tam_vocabulario_comun)\n",
    "print('Palabras totales dentro de la base de datos final: %d'%cant_palabras_comunes)\n",
    "\n",
    "# Ploteo el histograma\n",
    "bines_comunes = bines = histograma_diccionario(palabras_comunes, diccionario_comun, 0, PLOT = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalmente si seleccione 0 palabras de entrada calculo la cantidad de palabras de entrada \n",
    "# como la media de palabras por oración\n",
    "if palabras_entrada == 0:\n",
    "    palabras_entrada = round((np.array(largo_oraciones).mean()/2))\n",
    "    palabras_entrada = int(palabras_entrada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construcción Grafo TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En tesorflow la idea es primero armar un mapa de operaciones a realizar que se llama grafo. Este grafo esta compuesto por nodos que son operaciones y lineas que son tensores. Los tensores son matrices N dimencionales que llevan los datos. Una particularidad de tensorflow es que podemos definir tensores con una de sus dimenciones desconocidas, la cual se calcula en tiempo de ejecución. Un ejemplo de esto es el tamaño de batch. El batch es la cantidad de ejemplos que le damos a la red antes de actualizar sus pesos, este numero no tiene importancia para los cálculos de las derivadas del grafo, pero si a la hora de ejecutarlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero declaramos el punto de entrada.\n",
    "# La entrada es un tensor de tipo \"contenedor\". Es un tipo de dato que va a ser\n",
    "# cargado con los datos de entrada que le proveamos.\n",
    "# Tiene 3 dimenciones [batch_size , palabras_entrada , Caracteristicas]\n",
    "# El tamaño del bach no lo definimos aún, porque no es necesario, entonces le ponemos None\n",
    "# Si bien conocemos la cantidad de palabras de entrada que vamos a utilizar, elejimos ponerle \n",
    "# None. Esto es para mostrar la flexibilidad de las redes recursivas, como la celda se aplica\n",
    "# sobre cadad entrada pero tiene los pesos son compartidos. Es como si en realidad existiera solo \n",
    "# una celda que se aplica recursivamente (que casualidad que se llamen recursivas).\n",
    "x = tf.placeholder(\"float\", [None, None, 1])\n",
    "\n",
    "# El objetivo también es un contenedor donde vamos a poner el... objetivo...\n",
    "y = tf.placeholder(\"float\", [None, tam_vocabulario_comun])\n",
    "\n",
    "# Ahora declaro las celdas LSTM.\n",
    "# Los tf.name_scope son para poder darles un nombre y poder visualizarlas mejor en tensorboard\n",
    "with tf.name_scope('multi_LTSM'):    \n",
    "    with tf.name_scope('LTSM_1'):\n",
    "        lstm_cell_1 = tf.nn.rnn_cell.LSTMCell(num_units=LTMS_internal_units_1, state_is_tuple=True)\n",
    "\n",
    "    with tf.name_scope('LTSM_2'):\n",
    "        lstm_cell_2 = tf.nn.rnn_cell.LSTMCell(num_units=LTMS_internal_units_2, state_is_tuple=True)\n",
    "\n",
    "    # Una vez declaradas las dos celdas con sus respectivos elementos las conectamos de manera que\n",
    "    # la salida de la primer celda sea la entrada de la segunda. Por suerte TF hace esto por nosotros.\n",
    "    multi_lstm_cells = tf.contrib.rnn.MultiRNNCell(cells=[lstm_cell_1, lstm_cell_2] , state_is_tuple=True)\n",
    "\n",
    "# Ahora debemos indicarle a tensorflow que estas celdas son parte de una red recursiva. Esto se hace con \n",
    "# \"tf.nn.static_rnn\" o \"tf.nn.dynamic_rnn\". Estas funciones hacen casi lo mismo, solo que mientras la \n",
    "# ultima configura la red con un lazo (un while) la otra la \"hardcodea\". No hay diferencia en la velocidad\n",
    "# de ejecución y es posible que la estática sea descontinuada en el futuro, ya que es una verción de la \n",
    "# dinamica donde siempre el numero de itraciones es el mismo.\n",
    "with tf.name_scope('Dyn_RNN'):\n",
    "    output_rnn, state_rnn = tf.nn.dynamic_rnn(multi_lstm_cells, x, dtype=tf.float32)    \n",
    "    # output_rnn es la salida a cada t de la celda, con tamaño [batch_size, max_time, cell.output_size]\n",
    "    # state_rnn es el ultimo estado (t = T) de cada uno de los layers de celdas [num_celdas, batch_size, cell.state_size]\n",
    "\n",
    "# La salida que queremos nos la da state_rnn\n",
    "outputs = state_rnn\n",
    "# Más especificamente el estado de la última celda\n",
    "outputs = outputs[1]   \n",
    "\n",
    "# Luego de definir la celda dinamica se definen los tamaños de las matrices de pesos y bias\n",
    "# así que podemos setear los cálculos para el tensorboard.\n",
    "with tf.name_scope('multi_LTSM'):    \n",
    "    with tf.name_scope('LTSM_1'):\n",
    "        with tf.name_scope('Weights'):\n",
    "            variable_summaries(lstm_cell_1.weights[0])\n",
    "        with tf.name_scope('Bias'):\n",
    "            variable_summaries(lstm_cell_1.weights[1])\n",
    "    with tf.name_scope('LTSM_2'):\n",
    "        with tf.name_scope('Weights'):\n",
    "            variable_summaries(lstm_cell_2.weights[0])\n",
    "        with tf.name_scope('Bias'):\n",
    "            variable_summaries(lstm_cell_2.weights[1])\n",
    "\n",
    "# Ahora debemos interpretar el estado interno que nos dan las celdas como una palabra del diccionario.\n",
    "# Esto lo hacemos con una simple multiplicación de matrices optimizable o, si quieren sonar más pro, un \n",
    "# soft-maxed fully-connected layer (con esto matás en un boliche).\n",
    "# Primero declaramos los pesos y bias:\n",
    "with tf.name_scope('Matrix_Mult'):\n",
    "    with tf.name_scope('pesos_salida'):\n",
    "        weights = weight_variable([LTMS_internal_units_2, tam_vocabulario_comun]) \n",
    "        variable_summaries(weights)\n",
    "    with tf.name_scope('biases_salida'):\n",
    "        biases = bias_variable([tam_vocabulario_comun])\n",
    "        variable_summaries(biases)\n",
    "    \n",
    "    # Multiplicamos las matrices\n",
    "    out_tensor = tf.matmul(outputs[-1], weights) + biases\n",
    "    # Soft-max para todos y todos\n",
    "    soft_max_out_tensor = tf.nn.softmax(out_tensor)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Costo y optimización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aparte de describir la red debemos describir como es nuestra función de costo y como queremos optimizarla. Recordemos que la función de costo es la manera de decirle al optimizador que es lo que consideramos que esta bien o mal. En nuestro caso el costo es tratar de disminuir la diferencia entre la probabilidad de que a palabra que siga es la que propone la red y la realidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('cost'):\n",
    "# Acá definimos el costo, para más info https://en.wikipedia.org/wiki/Cross_entropy\n",
    "#   cost = tf.reduce_mean(-tf.reduce_sum(soft_max_out_tensor * tf.log(y), reduction_indices=[1]))\n",
    "# MUCHO MUY IMPORTANTE:\n",
    "# Como esto es un ejemplo se dividió el calculo del costo entre el soft-masx y la cross-entropy. \n",
    "# Esto no es recomendable por eso TF no nos provee un tf.nn.cross_entropy, sino que nos provee\n",
    "# una función que lo hace todo \"tf.nn.softmax_cross_entropy_with_logits\". Esta función es más\n",
    "# estable numericamente y está optimizada, de querer usarla es así:\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out_tensor, labels=y))\n",
    "# \"with_logits\" quiere decir que el vector de entrada no necesariamente suma 1.\n",
    "    tf.summary.scalar('cost', cost)\n",
    "\n",
    "# ¿Porque no usar la que escribimos? porque en el caso de que una palabra tenga probabilidad \n",
    "# cero de ser la corecta (algo que pasa con todas las palabras menos la correcta), la operacion\n",
    "# tf.log(y) --> -inf. Esto es un problema grave...\n",
    "\n",
    "# Una vez definido el costo definimos la función de optimización:\n",
    "#optimizer = tf.train.RMSPropOptimizer(learning_rate=velocidad_aprendizaje).minimize(cost)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=velocidad_aprendizaje).minimize(cost)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cálculos auxiliares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A veces es conveniente tener cálculos que nos muestren información que es útil par el humano pero que no tiene razon de ser para la optimización. Estos cálculos también deben ser declarados dentro del grafo.\n",
    "Tambien, y muy importante, se define como se va a guardar la información de la red entrenada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para saber cuantas palabras predecimos correctamente\n",
    "with tf.name_scope('accuracy'):\n",
    "    correct_pred = tf.equal(tf.argmax(soft_max_out_tensor,1), tf.argmax(y,1))\n",
    "    \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "\n",
    "# Declaramos la operación para guardar todo\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Operación de inicialización de variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicialización"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez todo definido se inicia la sessión de tensorflow, donde se construye el grafo. El tipo de sesión es \"interactiva\" lo que deja los recursos tomados por TF incluso despues del entrenamiento (útil para testear resultados).\n",
    "También aquí se incluyen las operaciones necesarias para que tensorboard nos dibuje el grafo y nos calcule la info sobre las variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activamos la sesión\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "# Inicializamos el grafo\n",
    "session.run(init)\n",
    "\n",
    "# Declaramos la salida del tensorboard y construimos la visualización\n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter('Tensorboard_out' + '/test',session.graph)\n",
    "train_writer.add_graph(session.graph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Break Teorico..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿como estan armados los tensores en tensorflow?\n",
    "\n",
    "La LSTM tiene 4 gates. La LTMS tiene un solo peso cuando esta en soledad, tiene un solo estado oculto. Cuando seteamos las unidades internas estamos agregando unidades que funcionan en paralelo y que se comunican a traves del estado oculto. El estado oculto (C en la bibliografia) tiene dimención la cantdad de unidades que existen. La salida h tambien tiene dimencion igual al total de unidades en paralelo.\n",
    "Cada gate debe actualizar que se recuerda y cada unidad lo hace para ella sola, pero teniendo en cuenta la información de todas. Es decir que si nuestra entrada tiene N elementos y ponemos D celdas LTMS, cada gate debe tener N+D pesos y D biases para poder tener en cuenta toda la información. Asi que seria un vector de 1 x (N+D) pesos por cada gate de cada celda. Entonces:\n",
    "\n",
    "Pesos del Gate * Vector concatenado de entrada + bias = decicion del gate para el estado oculto de esta celda (antes de la alinealidad)\n",
    "\n",
    "$\\begin{pmatrix}W^{x}_1 & W^{x}_2 & W^{x}_3 & ... & ... & W^{x}_{N+D} \\end{pmatrix} *  \\begin{pmatrix}X_1 \\\\X_2 \\\\X_3 \\\\... \\\\... \\\\X_{N} \\\\h^{t-1}_1 \\\\h^{t-1}_2 \\\\h^{t-1}_3 \\\\... \\\\... \\\\h^{t-1}_{D} \\end{pmatrix}  +  b^{x}_1 = G^{x}_1$\n",
    "\n",
    "Pero como esto se hace en cada estado oculto y tenemos tantos estados ocultos como celdas ( D ):\n",
    "\n",
    "\n",
    "$\\begin{pmatrix}W^{x}_{1,1} & W^{x}_{1,2} & W^{x}_{1,3} & ... & ... & ... & ... & W^{x}_{1,D} \\\\W^{x}_{2,1} & W^{x}_{2,2} & W^{x}_{2,3} & ... & ... & ... & ... & ... \\\\W^{x}_{3,1} & W^{x}_{3,2} & W^{x}_{3,3} & ... & ... & ... & ... & ... \\\\... & ... & ... & ... & ... & ... & ... & ... \\\\... & ... & ... & ... & ... & ... & ... & ... \\\\... & ... & ... & ... & ... & ... & ... & ... \\\\... & ... & ... & ... & ... & ... & ... & ... \\\\W^{x}_{D,1} & ... & ... & ... & ... & ... & ... & W^{x}_{D,N+D} \\\\\\end{pmatrix} * \\begin{pmatrix}X_1 \\\\X_2  \\\\... \\\\X_{N} \\\\h^{t-1}_1 \\\\h^{t-1}_2  \\\\... \\\\h^{t-1}_{D} \\end{pmatrix}+\\begin{pmatrix}b^{x}_1 \\\\b^{x}_2  \\\\... \\\\... \\\\... \\\\...  \\\\... \\\\b^{x}_{D} \\end{pmatrix}= \\begin{pmatrix}G^{x}_1 \\\\G^{x}_2  \\\\... \\\\... \\\\... \\\\...  \\\\... \\\\G^{x}_{D} \\end{pmatrix}$\n",
    "\n",
    "\n",
    "Y ademas tenemos 4 gates! El \"output gate\" (o), el \"forget gate\" (f) y \"input gate\" (i) que funciona junto con el otro gate que no encontre el nombre oficial ($\\widetilde{C}$, viene a ser como un estado interno).  En total cada aplicacion de las D celdas LTSM es una multiplicacion de matrices del siguiente tamaño $ [4xD  , (N+D)] * [(N+D) , 4xD] + [4xD , 1] = [4xD , 1] $. Distribuido asi:\n",
    "\n",
    "$\\begin{pmatrix}W_{1,1}^{o} & W_{1,2}^{o} & W_{1,3}^{o} & ... & ... & ... & ... & W_{1,D}^{o} \\\\W_{2,1}^{o} & W_{2,2}^{o} & W_{2,3}^{o} & ... & ... & ... & ... & ... \\\\W_{3,1}^{o} & W_{3,2}^{o} & W_{3,3}^{o} & ... & ... & ... & ... & ... \\\\... & ... & ... & ... & ... & ... & ... & ... \\\\... & ... & ... & ... & ... & ... & ... & ... \\\\... & ... & ... & ... & ... & ... & ... & ... \\\\... & ... & ... & ... & ... & ... & ... & ... \\\\W_{D,1}^{o} & ... & ... & ... & ... & ... & ... & W_{D,N+D}^{o} \\\\W_{1,1}^{i} & W_{1,2}^{i} & W_{1,3}^{i} & ... & ... & ... & ... & W_{1,D}^{i} \\\\W_{2,1}^{i} & W_{2,2}^{i} & W_{2,3}^{i} & ... & ... & ... & ... & ... \\\\W_{3,1}^{i} & W_{3,2}^{i} & W_{3,3}^{i} & ... & ... & ... & ... & ... \\\\... & ... & ... & ... & ... & ... & ... & ... \\\\... & ... & ... & ... & ... & ... & ... & ... \\\\... & ... & ... & ... & ... & ... & ... & ... \\\\... & ... & ... & ... & ... & ... & ... & ... \\\\W_{D,1}^{i} & ... & ... & ... & ... & ... & ... & W_{D,N+D}^{i} \\\\W_{1,1}^{\\widetilde{C}} & W_{1,2}^{\\widetilde{C}} & W_{1,3}^{\\widetilde{C}} & ... & ... & ... & ... & W_{1,D}^{\\widetilde{C}} \\\\W_{2,1}^{\\widetilde{C}} & W_{2,2}^{\\widetilde{C}} & W_{2,3}^{\\widetilde{C}} & ... & ... & ... & ... & ... \\\\W_{3,1}^{\\widetilde{C}} & W_{3,2}^{\\widetilde{C}} & W_{3,3}^{\\widetilde{C}} & ... & ... & ... & ... & ... \\\\... & ... & ... & ... & ... & ... & ... & ... \\\\... & ... & ... & ... & ... & ... & ... & ... \\\\... & ... & ... & ... & ... & ... & ... & ... \\\\... & ... & ... & ... & ... & ... & ... & ... \\\\W_{D,1}^{\\widetilde{C}} & ... & ... & ... & ... & ... & ... & W_{D,N+D}^{\\widetilde{C}} \\\\W_{1,1}^{f} & W_{1,2}^{f} & W_{1,3}^{f} & ... & ... & ... & ... & W_{1,D}^{f} \\\\W_{2,1}^{f} & W_{2,2}^{f} & W_{2,3}^{f} & ... & ... & ... & ... & ... \\\\W_{3,1}^{f} & W_{3,2}^{f} & W_{3,3}^{f} & ... & ... & ... & ... & ... \\\\... & ... & ... & ... & ... & ... & ... & ... \\\\... & ... & ... & ... & ... & ... & ... & ... \\\\... & ... & ... & ... & ... & ... & ... & ... \\\\... & ... & ... & ... & ... & ... & ... & ... \\\\W_{D,1}^{f} & ... & ... & ... & ... & ... & ... & W_{D,N+D}^{f} \\\\\\end{pmatrix}*\\begin{pmatrix}X_1 \\\\X_2  \\\\... \\\\X_{N} \\\\h^{t-1}_1 \\\\h^{t-1}_2  \\\\... \\\\h^{t-1}_{D} \\end{pmatrix}+\\begin{pmatrix}b^{o}_1\\\\b^{o}_2 \\\\... \\\\... \\\\... \\\\... \\\\... \\\\b^{o}_D \\\\b^{i}_1\\\\b^{i}_2 \\\\... \\\\... \\\\... \\\\... \\\\... \\\\b^{i}_D \\\\b^{\\widetilde{C}}_1\\\\b^{\\widetilde{C}}_2 \\\\... \\\\... \\\\... \\\\... \\\\... \\\\b^{\\widetilde{C}}_D \\\\b^{f}_1\\\\b^{f}_2 \\\\... \\\\... \\\\... \\\\... \\\\... \\\\b^{f}_D \\\\\\end{pmatrix}=\\begin{pmatrix}G^{o}_1\\\\G^{o}_2 \\\\... \\\\... \\\\... \\\\... \\\\... \\\\G^{o}_D \\\\G^{i}_1\\\\G^{i}_2 \\\\... \\\\... \\\\... \\\\... \\\\... \\\\G^{i}_D \\\\G^{\\widetilde{C}}_1\\\\G^{\\widetilde{C}}_2 \\\\... \\\\... \\\\... \\\\... \\\\... \\\\G^{\\widetilde{C}}_D \\\\G^{f}_1\\\\G^{f}_2 \\\\... \\\\... \\\\... \\\\... \\\\... \\\\G^{f}_D \\\\\\end{pmatrix}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "He aquí lo que vemos cuando nos metemos en el kernel de la RNN utilizando el tensor board.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿como estan armados los grafos de las celdas en tensorflow?\n",
    "\n",
    "Esta es mas dificil... No vas a ver las celdas LSTM una al lado de la otra en el tensorboard, sino que vas a ver un loop que genera automaticamente la cantidad de celdas necesarias dependiendo del largo del input. Mas info en:\n",
    "\n",
    "http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿como se entrena todo esto?\n",
    "\n",
    "Aunque sea muy grande, todo la celda (y el conjunto de celdas) es derivable y se puede usar desde backpropagation through time (Que es backpropagation normal solo que saben que cada estado depende del anterior por medio de $h_{t-1}$) hasta cosas mas complicadas como ADAM. Gacias a dios para esto existe tensorflow, podriamos derivar el backpropagation para una celda, pero no seria tan eficiente y tomaría mucho mas tiempo. Queda de tarea para el hogar..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting optmization...\n",
      "Iter= 5000, Costo Promedio = 5.445216, Presición promedio= 9.96%\n",
      "['this', 'weekend', ','] - [so] vs [the]\n",
      "Iter= 10000, Costo Promedio = 4.293132, Presición promedio= 10.52%\n",
      "[',', 'space', 'station'] - [.] vs [.]\n",
      "Iter= 15000, Costo Promedio = 2.886572, Presición promedio= 24.52%\n",
      "['another', 'level', ','] - [will] vs [yes]\n",
      "Iter= 20000, Costo Promedio = 2.918428, Presición promedio= 31.68%\n",
      "['exactly', '!', 'i'] - [love] vs [love]\n",
      "Iter= 25000, Costo Promedio = 4.348275, Presición promedio= 18.98%\n",
      "['too', ':', ')'] - [will] vs [.]\n",
      "Iter= 30000, Costo Promedio = 3.235521, Presición promedio= 30.20%\n",
      "['yeah', ',', 'exactly'] - [.] vs [.]\n",
      "Iter= 35000, Costo Promedio = 3.483490, Presición promedio= 25.74%\n",
      "['.', 'exactly', 'still'] - [on] vs [.]\n",
      "Iter= 40000, Costo Promedio = 3.593907, Presición promedio= 23.60%\n",
      "['me', 'what', 'you'] - [say] vs [.]\n",
      "Iter= 45000, Costo Promedio = 3.730363, Presición promedio= 17.02%\n",
      "['target', 'orbit', '.'] - [all] vs [cool]\n",
      "Iter= 50000, Costo Promedio = 3.299505, Presición promedio= 26.14%\n",
      "[')', 'i', 'agree'] - [.] vs [.]\n",
      "Iter= 55000, Costo Promedio = 2.847715, Presición promedio= 32.80%\n",
      "['months', '.', 'droneship'] - [is] vs [is]\n",
      "Iter= 60000, Costo Promedio = 2.939687, Presición promedio= 28.36%\n",
      "['part', '1', 'of'] - [the] vs [2]\n",
      "Iter= 65000, Costo Promedio = 3.023728, Presición promedio= 33.00%\n",
      "[',', 'no', 'point'] - [hoping] vs [hoping]\n",
      "Iter= 70000, Costo Promedio = 2.657611, Presición promedio= 38.38%\n",
      "['i', 'am', 'actually'] - [going] vs [going]\n",
      "Iter= 75000, Costo Promedio = 3.843496, Presición promedio= 16.98%\n",
      "['looks', 'like', 'we'] - [are] vs [.]\n",
      "Iter= 80000, Costo Promedio = 2.843848, Presición promedio= 28.80%\n",
      "['feel', 'exactly', 'the'] - [same] vs [car]\n",
      "Iter= 85000, Costo Promedio = 3.487982, Presición promedio= 20.34%\n",
      "['to', 'find', 'them'] - [y] vs [.]\n",
      "Iter= 90000, Costo Promedio = 1.782040, Presición promedio= 48.54%\n",
      "['issue', 'with', 'dragon'] - [thruster] vs [all]\n",
      "Iter= 95000, Costo Promedio = 3.814743, Presición promedio= 13.18%\n",
      "['a', 'chance', 'on'] - [given] vs [dragon]\n",
      "Iter= 100000, Costo Promedio = 2.889547, Presición promedio= 24.20%\n",
      "['of', 'course', ')'] - [...] vs [)]\n",
      "Iter= 105000, Costo Promedio = 1.628724, Presición promedio= 53.12%\n",
      "['3', 'unveil', 'will'] - [be] vs [3]\n",
      "Iter= 110000, Costo Promedio = 2.645147, Presición promedio= 34.08%\n",
      "['find', 'it', 'enough'] - [.] vs [.]\n",
      "Iter= 115000, Costo Promedio = 2.590810, Presición promedio= 34.34%\n",
      "['out', 'to', 'all'] - [model] vs [.]\n",
      "Iter= 120000, Costo Promedio = 3.092338, Presición promedio= 21.20%\n",
      "[':', ')', 'minor'] - [improvements] vs [improvements]\n",
      "Iter= 125000, Costo Promedio = 2.081147, Presición promedio= 42.40%\n",
      "['yes', 'maybe', 'a'] - [few] vs [few]\n",
      "Iter= 130000, Costo Promedio = 2.162120, Presición promedio= 41.34%\n",
      "['.', 'makes', 'a'] - [big] vs [tesla]\n",
      "Iter= 135000, Costo Promedio = 1.863251, Presición promedio= 45.24%\n",
      "['car', '(', 'those'] - [are] vs [.]\n",
      "Iter= 140000, Costo Promedio = 2.218405, Presición promedio= 43.20%\n",
      "['tesla', 'will', 'do'] - [something] vs [3]\n",
      "Iter= 145000, Costo Promedio = 1.886294, Presición promedio= 48.80%\n",
      "['that', '.', 'dragon'] - [abort] vs [,]\n",
      "Iter= 150000, Costo Promedio = 1.044472, Presición promedio= 67.36%\n",
      "['all', 'cars', '.'] - [yes] vs [part]\n",
      "Iter= 155000, Costo Promedio = 2.569513, Presición promedio= 32.56%\n",
      "['care', 'of', 'kinda'] - [.] vs [.]\n",
      "Iter= 160000, Costo Promedio = 1.586649, Presición promedio= 53.30%\n",
      "['mars', '.', 'some'] - [years] vs [others]\n",
      "Iter= 165000, Costo Promedio = 2.010512, Presición promedio= 46.92%\n",
      "['of', 'those', 'too'] - [good] vs [yup]\n",
      "Iter= 170000, Costo Promedio = 2.126891, Presición promedio= 43.52%\n",
      "['great', '!', 'a'] - [lot] vs [lot]\n",
      "Iter= 175000, Costo Promedio = 2.045969, Presición promedio= 43.16%\n",
      "['have', 'to', ':'] - [)] vs [)]\n",
      "Iter= 180000, Costo Promedio = 2.044284, Presición promedio= 43.62%\n",
      "['was', 'this', 'in'] - [a] vs [the]\n",
      "Iter= 185000, Costo Promedio = 1.624268, Presición promedio= 53.40%\n",
      "['yes', ',', 'confident'] - [that] vs [several]\n",
      "Iter= 190000, Costo Promedio = 1.957021, Presición promedio= 45.14%\n",
      "['year', 'that', 'is'] - [true] vs [true]\n",
      "Iter= 195000, Costo Promedio = 1.491504, Presición promedio= 57.18%\n",
      "['have', 'a', 'great'] - [experience] vs [move]\n",
      "Iter= 200000, Costo Promedio = 1.635501, Presición promedio= 53.46%\n",
      "['to', 'do', '.'] - [something] vs [something]\n",
      "Iter= 205000, Costo Promedio = 1.531378, Presición promedio= 55.82%\n",
      "['good', 'idea', 'yes'] - [also] vs [also]\n",
      "Iter= 210000, Costo Promedio = 1.503296, Presición promedio= 55.90%\n",
      "['of', 'course', 'maybe'] - [one] vs [one]\n",
      "Iter= 215000, Costo Promedio = 1.627111, Presición promedio= 54.38%\n",
      "['was', 'great', '!'] - [a] vs [i]\n",
      "Iter= 220000, Costo Promedio = 1.228901, Presición promedio= 63.24%\n",
      "['a', 'big', 'difference'] - [if] vs [if]\n",
      "Iter= 225000, Costo Promedio = 1.461627, Presición promedio= 57.62%\n",
      "['upgraded', ')', 'or'] - [a] vs [to]\n",
      "Iter= 230000, Costo Promedio = 0.917364, Presición promedio= 72.04%\n",
      "['is', 'why', 'i'] - [think] vs [there]\n",
      "Iter= 235000, Costo Promedio = 1.632228, Presición promedio= 52.60%\n",
      "['future', 'to', 'be'] - [good] vs [good]\n",
      "Iter= 240000, Costo Promedio = 1.056888, Presición promedio= 68.06%\n",
      "['cover', 'whole', 'country'] - [yes] vs [.]\n",
      "Iter= 245000, Costo Promedio = 1.954017, Presición promedio= 45.72%\n",
      "['autopilot', 'by', 'autopilot'] - [already] vs [either]\n",
      "Iter= 250000, Costo Promedio = 1.839726, Presición promedio= 49.48%\n",
      "[',', 'mars', '.'] - [some] vs [some]\n",
      "Iter= 255000, Costo Promedio = 1.590696, Presición promedio= 54.04%\n",
      "['of', 'those', 'too'] - [good] vs [good]\n",
      "Iter= 260000, Costo Promedio = 1.196416, Presición promedio= 64.66%\n",
      "['yes', 'maybe', 'a'] - [few] vs [few]\n",
      "Iter= 265000, Costo Promedio = 1.255424, Presición promedio= 62.86%\n",
      "['have', 'to', ':'] - [)] vs [)]\n",
      "Iter= 270000, Costo Promedio = 1.392041, Presición promedio= 59.38%\n",
      "['a', 'test', 'drive'] - [car] vs [car]\n",
      "Iter= 275000, Costo Promedio = 1.199550, Presición promedio= 65.48%\n",
      "['appreciated', 'yes', ','] - [confident] vs [confident]\n",
      "Iter= 280000, Costo Promedio = 1.228705, Presición promedio= 62.80%\n",
      "['.', 'end', 'of'] - [year] vs [year]\n",
      "Iter= 285000, Costo Promedio = 1.638501, Presición promedio= 55.62%\n",
      "['i', 'hope', 'you'] - [have] vs [too]\n",
      "Iter= 290000, Costo Promedio = 1.412623, Presición promedio= 58.98%\n",
      "['right', 'thing', 'to'] - [do] vs [do]\n",
      "Iter= 295000, Costo Promedio = 0.770551, Presición promedio= 74.96%\n",
      "['in', 'next', 'few'] - [days] vs [days]\n",
      "Iter= 300000, Costo Promedio = 1.516539, Presición promedio= 59.06%\n",
      "['is', 'not', 'true'] - [.] vs [.]\n",
      "¡Optimización finalizada!\n",
      "('Tiempo transcurrido: ', 38244.6728720665)\n",
      "Modelo final guardado en archivo: ./Salidas/elonmusk_tweets_Final_bot.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Nos quedamos con el timestap de inicio para medir tiempo\n",
    "start_time = time.time()\n",
    "\n",
    "# Inicializamos variables\n",
    "step = 0\n",
    "offset = random.randint(0,palabras_entrada+1)\n",
    "end_offset = palabras_entrada + 1\n",
    "acc_total = 0\n",
    "loss_total = 0\n",
    "\n",
    "print(\"Starting optmization...\")\n",
    "# Mientras queramos...\n",
    "while step < iteraciones_entrenamiento:\n",
    "\n",
    "    symbols_in_keys = np.zeros((tam_minibatch,palabras_entrada,1))\n",
    "    symbols_out_onehot = np.zeros((tam_minibatch,tam_vocabulario_comun))\n",
    "    for i_batch in range(0,tam_minibatch):\n",
    "\n",
    "        # Generamos un minibatch empezando de un punto al azar\n",
    "        if offset > (len(palabras_comunes)-end_offset):\n",
    "            # En caso de haberle dado una vuelta completa a nuestra base de datos, comenzamos nuevamente\n",
    "            offset = random.randint(0, palabras_entrada+1)\n",
    "\n",
    "        # Copiamos de la base de datos la cantidad de palabras elegidas a un vector codificado\n",
    "        symbols_in_keys_aux = [ [diccionario_comun[ str(palabras_comunes[i])]] for i in range(offset, offset+palabras_entrada) ]\n",
    "        # Lo transformamos en un batch con la forma del tensor de entrada\n",
    "        symbols_in_keys_aux = np.reshape(np.array(symbols_in_keys_aux), [-1, palabras_entrada, 1])\n",
    "        symbols_in_keys[i_batch, : , :] = symbols_in_keys_aux\n",
    "\n",
    "        # Para la salida (verdad objetivo) armamos un vector lleno de ceros\n",
    "        symbols_out_onehot_aux = np.zeros([tam_vocabulario_comun], dtype=float)\n",
    "        # Y al código de la palabra que sigue a las alimentadas a la red, le asignamos 1 (100% de probabilidad)\n",
    "        symbols_out_onehot_aux[diccionario_comun[str(palabras_comunes[offset+palabras_entrada])]] = 1.0\n",
    "        # Y lo transformamos en la dimensiín del placeholder de salida\n",
    "        symbols_out_onehot_aux = np.reshape(symbols_out_onehot_aux,[1,-1])\n",
    "        symbols_out_onehot[i_batch, :] = symbols_out_onehot_aux;\n",
    "\n",
    "    # Corremos los cálculos:\n",
    "    # --- optimizar (optimización de la red)\n",
    "    # --- accuracy (medición para mostrar)\n",
    "    # --- cost (valor de la función de costos para el estado actual)\n",
    "    # --- soft_max_out_tensor (valor de la salida de la red para el último paso)\n",
    "    # --- merged (calculo de información sobre variables a optimizar - TensorBoard)\n",
    "    # Y le pasamos como entrada a los placeholders que definimos:\n",
    "    # --- symbols_in_keys\n",
    "    # --- symbols_out_onehot\n",
    "    _, acc, loss, onehot_pred, summary = session.run([optimizer, accuracy, cost, soft_max_out_tensor, merged], \\\n",
    "                                            feed_dict={x: symbols_in_keys, y: symbols_out_onehot})\n",
    "    # Agrego la info al tensorboard si corresponde\n",
    "    if (step+1) % procesar_info_cada == 0:\n",
    "        train_writer.add_summary(summary, step)\n",
    "    # Vamos sumando el costo y la presición para hacer luego un promedio\n",
    "    loss_total += loss\n",
    "    acc_total += acc\n",
    "    # Y checkeamos si es el paso en el que vamos a mostrar todo\n",
    "    if (step+1) % mostrar_info_cada == 0:\n",
    "        # De ser así imprimimos la info\n",
    "        print(\"Iter= \" + str(step+1) + \", Costo Promedio = \" + \\\n",
    "              \"{:.6f}\".format(loss_total/mostrar_info_cada) + \", Presición promedio= \" + \\\n",
    "              \"{:.2f}%\".format(100*acc_total/mostrar_info_cada))\n",
    "        # Reseteamos promediadores\n",
    "        acc_total = 0\n",
    "        loss_total = 0\n",
    "        # Recuperamos las palabras usadas en el bach\n",
    "        symbols_in = [palabras_comunes[i] for i in range(offset, offset + palabras_entrada)]\n",
    "        symbols_out = palabras_comunes[offset + palabras_entrada]\n",
    "        # Las transformamos de codigo en palabras \n",
    "        symbols_out_pred = diccionario_inverso_comun[int(tf.argmax(np.reshape(onehot_pred[0],[1,-1]), 1).eval())]\n",
    "        # Mostramos la última predicción\n",
    "        print(\"%s - [%s] vs [%s]\" % (symbols_in,symbols_out,symbols_out_pred))\n",
    "        \n",
    "        # Guardamos el paso intermedio\n",
    "        saver.save(session, \"./Salidas/%s_%d_bot.ckpt\"%(ARCH_TWEETS.rpartition('.')[0],step+1))\n",
    "    # Update del contador de pasos\n",
    "    step += 1\n",
    "    # Nos corremos en el indice de entrada de palabrasm una palabra para adelante\n",
    "    offset += (palabras_entrada+1)\n",
    "# Termino!\n",
    "print(\"¡Optimización finalizada!\")\n",
    "print(\"Tiempo transcurrido: \", time.time() - start_time)\n",
    "\n",
    "# Guardamos iteración final\n",
    "MODELO_RESTORE = \"./Salidas/elonmusk_tweets_Final_bot.ckpt\"\n",
    "save_path = saver.save(session, MODELO_RESTORE)\n",
    "print(\"Modelo final guardado en archivo: %s\" % save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Producción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos cargar el modelo ya entrenado y ver que tiene para decir.\n",
    "No es necesario cargar si acabamos de entrenar, pero está buena la práctica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./Salidas/elonmusk_tweets_Final_bot.ckpt\n",
      "Restaurado modelo: ./Salidas/elonmusk_tweets_Final_bot.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Para alarar, esto lo podes correr sin haber ejecutado el entrenamiento, pero si tenes que \n",
    "# ejecutar todo el codigo que arma el grafo. Lo que se guarda son solo las variables.\n",
    "saver.restore(session, MODELO_RESTORE)\n",
    "\n",
    "print(\"Restaurado modelo: %s\"%MODELO_RESTORE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 words: 'no car great'\n",
      "no car great just . working ! seems . ok ! it\n",
      "3 words: 'no rocket car '\n",
      "no rocket car and yeah read are . will make that change\n",
      "3 words: 'will space rocket'\n",
      "will space rocket autopilot . i do it still agree on norway\n",
      "3 words: 'and car autopilot'\n",
      "and car autopilot kinda . you is not true . yeah can\n",
      "3 words: 'spacex and tesla'\n",
      "spacex and tesla , then very through . just soon . almost\n",
      "3 words: 'no way no'\n",
      "no way no yeah , yes it yeah , movie a probably\n"
     ]
    }
   ],
   "source": [
    "PALABRAS_A_GENERAR = 9\n",
    "\n",
    "while True:\n",
    "    prompt = \"%s words: \" % palabras_entrada\n",
    "    sentence = input(prompt)\n",
    "    sentence = sentence.strip()\n",
    "    words = sentence.split(' ')\n",
    "    if len(words) != palabras_entrada:\n",
    "        continue\n",
    "\n",
    "    symbols_in_keys = [diccionario_comun[str(words[i])] for i in range(len(words))]\n",
    "    for i in range(PALABRAS_A_GENERAR):\n",
    "        keys = np.reshape(np.array(symbols_in_keys), [-1, palabras_entrada, 1])\n",
    "        onehot_pred = session.run(soft_max_out_tensor, feed_dict={x: \n",
    "                                                                  keys})\n",
    "        onehot_pred_index = int(tf.argmax(onehot_pred, 1).eval())\n",
    "        sentence = \"%s %s\" % (sentence,diccionario_inverso_comun[onehot_pred_index])\n",
    "        symbols_in_keys = symbols_in_keys[1:]\n",
    "        symbols_in_keys.append(onehot_pred_index)\n",
    "    print(sentence)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
